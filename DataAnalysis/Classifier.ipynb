{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:25:06.840076Z",
     "start_time": "2020-07-16T15:24:42.312220Z"
    }
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from JSAnimation import IPython_display\n",
    "from matplotlib import animation\n",
    "import matplotlib.patches as mpatches\n",
    "import cv2\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as c_layers\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import glob\n",
    "from scipy import misc\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:25:12.332992Z",
     "start_time": "2020-07-16T15:25:06.911884Z"
    }
   },
   "outputs": [],
   "source": [
    "all_classes = ['[0, 0, 0, 0]','[0, 0, 0, 1]','[0, 0, 1, 0]','[0, 1, 0, 0]','[0, 1, 1, 0]','[1, 0, 0, 0]',\n",
    "               '[1, 0, 1, 0]','[2, 0, 0, 0]','[2, 0, 0, 1]','[2, 0, 1, 0]','[2, 1, 0, 0]',#'[2, 1, 1, 0]','[2,0,1,1]'\n",
    "               '[3, 0, 0, 0]','[3, 0, 0, 1]','[3, 0, 1, 0]','[3, 1, 0, 0]','[4, 0, 0, 0]','[4, 0, 0, 1]',\n",
    "               '[4, 0, 1, 0]']\n",
    "datagen = ImageDataGenerator(validation_split=0.1,rescale=1./255)\n",
    "train_it = datagen.flow_from_directory('./Results/TowerTraining/Classifier/Sorted/', class_mode='sparse',\n",
    "                                       batch_size=256,shuffle=True,subset=\"training\",target_size=(168,168),classes=all_classes)\n",
    "val_it = datagen.flow_from_directory('./Results/TowerTraining/Classifier/Sorted/', class_mode='sparse',\n",
    "                                       batch_size=256,shuffle=True,subset=\"validation\",target_size=(168,168),classes=all_classes)\n",
    "\n",
    "realLabel = []\n",
    "for c,v in train_it.class_indices.items():\n",
    "    c_ext = np.fromstring(c[1:-1], dtype=int, sep=', ')\n",
    "    realLabel.append(c_ext)\n",
    "\n",
    "def getRealLabel(labelBatch,RL):\n",
    "    newLB = []\n",
    "    for label in labelBatch:\n",
    "        l = RL[int(label)]\n",
    "        newLB.append(l)\n",
    "    return newLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:25:12.573280Z",
     "start_time": "2020-07-16T15:25:12.546282Z"
    }
   },
   "outputs": [],
   "source": [
    "num_exp = [[0,0,0,0,0],[0,0],[0,0],[0,0]]\n",
    "classes = [[[0,1,2,3,4],[5,6],[7,8,9,10],[11,12,13,14],[15,16,17]],[[0,1,2,5,6,7,8,9,11,12,13,15,16,17],[3,4,10,14]],\n",
    "           [[0,1,3,5,7,8,10,11,12,14,15,16],[2,4,6,9,13,17]],[[0,2,3,4,5,6,7,9,10,11,13,14,15,17],[1,8,12,16]]]\n",
    "for branch in range(len(num_exp)):\n",
    "    for c in range(len(num_exp[branch])):\n",
    "        sum_exp = 0\n",
    "        for s in classes[branch][c]:\n",
    "            sum_exp = sum_exp + train_it.classes[train_it.classes==s].shape[0]\n",
    "        num_exp[branch][c] = sum_exp\n",
    "print('Number of samples: '+str(num_exp))\n",
    "if np.sum(num_exp[0])== np.sum(num_exp[1]) == np.sum(num_exp[2]) == np.sum(num_exp[3]):\n",
    "    print('all good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:25:12.728249Z",
     "start_time": "2020-07-16T15:25:12.713244Z"
    }
   },
   "outputs": [],
   "source": [
    "class_weights = [[0,0,0,0,0],[0,0],[0,0],[0,0]]\n",
    "print('Class Weights: ')\n",
    "for branch in range(len(class_weights)):\n",
    "    bincount = np.array(num_exp[branch])\n",
    "    weights = np.sum(bincount) / (bincount.shape[0] * bincount)\n",
    "    class_weights[branch] = weights\n",
    "    print(np.round(weights,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:25:12.852158Z",
     "start_time": "2020-07-16T15:25:12.844180Z"
    }
   },
   "outputs": [],
   "source": [
    "train_it.class_indices.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:25:13.023965Z",
     "start_time": "2020-07-16T15:25:13.009004Z"
    }
   },
   "outputs": [],
   "source": [
    "'''@misc{guest2017gini,\n",
    "  author = \"Olivia Guest\",\n",
    "  title = \"Using the Gini Coefficient to Evaluate Deep Neural Network Layer Representations\",\n",
    "  year = \"2017\",\n",
    "  howpublished = \"Blog post\",\n",
    "  url = \"http://neuroplausible.com/gini\"\n",
    "}'''\n",
    "def gini(array):\n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"\n",
    "    # based on bottom eq: http://www.statsdirect.com/help/content/image/stat0206_wmf.gif\n",
    "    # from: http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "    array = array.flatten() #all values are treated equally, arrays must be 1d\n",
    "    #array = np.abs(array)# change from original code\n",
    "    #if np.amin(array) < 0:\n",
    "     #   array -= np.amin(array) #values cannot be negative\n",
    "    array += 0.0000001 #values cannot be 0\n",
    "    array = np.sort(array) #values must be sorted\n",
    "    index = np.arange(1,array.shape[0]+1) #index per array element\n",
    "    n = array.shape[0]#number of array elements\n",
    "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array))) #Gini coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:25:21.185462Z",
     "start_time": "2020-07-16T15:25:13.245373Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./Results/TowerTraining/Recordings/Standard/3999_16.100/\"\n",
    "obs = np.load(path+\"visobs.npy\")\n",
    "hand_l = pd.read_csv(path+'HandLabels.csv')\n",
    "label_test = np.zeros(obs.shape[0])\n",
    "label_test = np.array(hand_l['Label'])\n",
    "def addLabel(currL,ToAdd):\n",
    "    if ToAdd<=1:\n",
    "        return currL\n",
    "    elif ToAdd==2:\n",
    "        currL[0] = 4\n",
    "    elif ToAdd==3:\n",
    "        currL[0] = 2\n",
    "    elif ToAdd==4:\n",
    "        currL[0] = 3\n",
    "    elif ToAdd==5:\n",
    "        currL[0] = 4\n",
    "    elif ToAdd==6:\n",
    "        currL[0]=1\n",
    "    elif ToAdd==7:\n",
    "        currL[1]=1\n",
    "    elif ToAdd==8:\n",
    "        currL[2]=1\n",
    "    elif ToAdd==9:\n",
    "        currL[3]=1\n",
    "    else:\n",
    "        print(ToAdd)\n",
    "    return currL\n",
    "\n",
    "def formatLabels(labels1,labels2):\n",
    "    formatted = []\n",
    "    for i,l in enumerate(labels1):\n",
    "        newLabel=[0,0,0,0]\n",
    "        if l == 0:\n",
    "            print(i)\n",
    "        newLabel = addLabel(newLabel,l)\n",
    "        #print(str(i)+': '+str(newLabel)+ '  '+str(l))\n",
    "        newLabel = addLabel(newLabel,labels2[i])\n",
    "        #print(str(i)+': '+str(newLabel)+ '  '+str(labels2[i]))\n",
    "        formatted.append(newLabel)\n",
    "    return np.array(formatted)\n",
    "\n",
    "fLabels = formatLabels(np.array(hand_l['Label']),np.array(hand_l['Secondary label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Classification\n",
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T11:39:32.600569Z",
     "start_time": "2020-07-17T11:39:27.917154Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def swish(input_activation):\n",
    "    \"\"\"Swish activation function. For more info: https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    return tf.multiply(input_activation, tf.nn.sigmoid(input_activation))\n",
    "\n",
    "o_size_h = 168\n",
    "o_size_w = 168\n",
    "vec_obs_size = 8\n",
    "num_layers = 2\n",
    "h_size = 256\n",
    "h_size_vec = 256\n",
    "            \n",
    "visual_in = tf.placeholder(shape=[None, o_size_h, o_size_w, 3], dtype=tf.float32,name=\"visual_observation_0\")\n",
    "labels = tf.placeholder(shape=[None,4], dtype=tf.int64,name=\"labels\")\n",
    "\n",
    "def create_vector_observation_encoder(observation_input, h_size, activation, num_layers, scope,reuse):\n",
    "    with tf.variable_scope(scope):\n",
    "        hidden_vec = observation_input\n",
    "        for i in range(num_layers):\n",
    "            hidden_vec = tf.layers.dense(hidden_vec, h_size, activation=activation, reuse=reuse,name=\"hidden_{}\".format(i),kernel_initializer=c_layers.variance_scaling_initializer(1.0))\n",
    "    return hidden_vec\n",
    "\n",
    "def create_visual_observation_encoder(image_input, h_size, activation, num_layers, scope,reuse):\n",
    "    with tf.variable_scope(scope):\n",
    "        conv1 = tf.layers.conv2d(image_input, 16, kernel_size=[8, 8], strides=[4, 4],activation=tf.nn.elu, reuse=reuse, name=\"conv_1\")\n",
    "        conv2 = tf.layers.conv2d(conv1, 32, kernel_size=[4, 4], strides=[2, 2],activation=tf.nn.elu, reuse=reuse, name=\"conv_2\")\n",
    "        hidden_vis = c_layers.flatten(conv2)\n",
    "\n",
    "    with tf.variable_scope(scope + '/' + 'flat_encoding'):\n",
    "        hidden_flat = create_vector_observation_encoder(hidden_vis, h_size, activation,num_layers, scope, reuse)\n",
    "    return hidden_flat\n",
    "\n",
    "def create_discrete_action_masking_layer(all_logits, action_size):\n",
    "        \"\"\"\n",
    "        Creates a masking layer for the discrete actions\n",
    "        :param all_logits: The concatenated unnormalized action probabilities for all branches\n",
    "        :param action_size: A list containing the number of possible actions for each branch\n",
    "        :return: The action output dimension [batch_size, num_branches] and the concatenated normalized logits\n",
    "        \"\"\"\n",
    "        action_idx = [0] + list(np.cumsum(action_size))\n",
    "        branches_logits = [all_logits[:, action_idx[i]:action_idx[i + 1]] for i in range(len(action_size))]#split in action branches (size=action_size)\n",
    "        raw_probs = [tf.nn.softmax(branches_logits[k]) + 1.0e-10 for k in range(len(action_size))]\n",
    "        normalized_probs = [\n",
    "            tf.divide(raw_probs[k], tf.reduce_sum(raw_probs[k], axis=1, keepdims=True))\n",
    "            for k in range(len(action_size))]\n",
    "        #normalized_probs = raw_probs\n",
    "        output = tf.concat([tf.multinomial(tf.log(normalized_probs[k]), 1) for k in range(len(action_size))], axis=1)#sample outputs from log probdist\n",
    "        #output = tf.concat([tf.multinomial(tf.nn.log_softmax(raw_probs[k]), 1) for k in range(len(action_size))], axis=1)#sample outputs from log probdist\n",
    "        \n",
    "        log_probs = [tf.log(normalized_probs[k] + 1.0e-10) for k in range(len(action_size))]#xx\n",
    "        log_probs_flat = tf.concat(log_probs, axis=1)\n",
    "        return output, log_probs_flat, log_probs\n",
    "\n",
    "visual_encoders = []\n",
    "\n",
    "encoded_visual = create_visual_observation_encoder(visual_in,h_size,swish,num_layers,\"main_graph_0_encoder0\", False)\n",
    "visual_encoders.append(encoded_visual)\n",
    "hidden = tf.concat(visual_encoders, axis=1)\n",
    "\n",
    "class_size = [5,2,2,2]\n",
    "policy_branches = []\n",
    "for size in class_size:\n",
    "    policy_branches.append(tf.layers.dense(hidden, size, activation=tf.nn.relu, use_bias=False,kernel_initializer=c_layers.variance_scaling_initializer(factor=0.01)))\n",
    "\n",
    "all_log_probs = tf.concat([branch for branch in policy_branches], axis=1, name=\"action_probs\")\n",
    "\n",
    "output, normalized_logits_flat, norm_logits = create_discrete_action_masking_layer(all_log_probs, class_size)\n",
    "\n",
    "output = tf.identity(output)\n",
    "normalized_logits = tf.identity(normalized_logits_flat, name='action')#has nan in places where prob is negative bc it it log(probs)\n",
    "\n",
    "comparison = tf.equal(labels, output)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(comparison, dtype=tf.float32))\n",
    "\n",
    "cross_entropiesD = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = norm_logits[0], labels = labels[:,0])\n",
    "cross_entropiesK = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = norm_logits[1], labels = labels[:,1])\n",
    "cross_entropiesO = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = norm_logits[2], labels = labels[:,2])\n",
    "cross_entropiesP = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = norm_logits[3], labels = labels[:,3])\n",
    "print(cross_entropiesD)\n",
    "\n",
    "class_weightsD = tf.constant(class_weights[0],dtype='float32')\n",
    "class_weightsK = tf.constant(class_weights[1],dtype='float32')\n",
    "class_weightsO = tf.constant(class_weights[2],dtype='float32')\n",
    "class_weightsP = tf.constant(class_weights[3],dtype='float32')\n",
    "print(class_weightsD)\n",
    "\n",
    "weightsD = tf.reduce_sum(class_weightsD * tf.one_hot(labels[:,0],5), axis=1)\n",
    "weightsK = tf.reduce_sum(class_weightsK * tf.one_hot(labels[:,1],2), axis=1)\n",
    "weightsO = tf.reduce_sum(class_weightsO * tf.one_hot(labels[:,2],2), axis=1)\n",
    "weightsP = tf.reduce_sum(class_weightsP * tf.one_hot(labels[:,3],2), axis=1)\n",
    "print(weightsD)\n",
    "scaled_errorD = cross_entropiesD * weightsD\n",
    "scaled_errorK = cross_entropiesK * weightsK\n",
    "scaled_errorO = cross_entropiesO * weightsO\n",
    "scaled_errorP = cross_entropiesP * weightsP\n",
    "\n",
    "cross_entropies = tf.reduce_mean([cross_entropiesD,cross_entropiesK,cross_entropiesO,cross_entropiesP],axis=1)\n",
    "print(cross_entropies)\n",
    "scaled_error = tf.reduce_mean([scaled_errorD,scaled_errorK,scaled_errorO,scaled_errorP],axis=1)\n",
    "# Calculate the mean cross entropy for the mini-batch \n",
    "mean_cross_entropy = tf.reduce_mean(cross_entropies)\n",
    "mean_scaled_error = tf.reduce_mean(scaled_error)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.0001)\n",
    "\n",
    "gradients = optimizer.compute_gradients(mean_scaled_error)\n",
    "hidden_grad = tf.gradients(mean_scaled_error,hidden)\n",
    "\n",
    "training_step = optimizer.minimize(mean_scaled_error)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "Allentropies = []\n",
    "Allaccuracies = []\n",
    "Allscaled_errs = []\n",
    "AllValaccuracies = []\n",
    "AllGinis = []\n",
    "LastSave = 0\n",
    "\n",
    "print('            Entropy    Error   Accuracy')\n",
    "epochs = 300\n",
    "for i in range(epochs):\n",
    "    count = 0\n",
    "    xVal,yVal = val_it.next()\n",
    "    yVal = getRealLabel(yVal,realLabel)\n",
    "    Valentro, Valsc_err, Valaccu,Valcomp,Valout,ValProbs = sess.run([mean_cross_entropy, mean_scaled_error,accuracy, comparison,output,normalized_logits_flat], feed_dict = {visual_in: xVal, labels: yVal})\n",
    "    #print(Valcomp[:10])\n",
    "    #print(yVal[:10])\n",
    "    #print(Valout[:10])\n",
    "    #print(np.round(ValProbs[:10],3))\n",
    "    if i==0:\n",
    "        print('Validation:  '+str(Valentro)+ ' - '+str(Valsc_err)+' - '+str(Valaccu))\n",
    "        AllValaccuracies.append(Valaccu)\n",
    "    else:\n",
    "        print('Training: '+str(np.mean(entropies))+' - '+str(np.mean(scaled_errs))+' - '+str(np.mean(accuracies))+\n",
    "          '    Validation:  '+str(Valentro)+ ' - '+str(Valsc_err)+' - '+str(Valaccu))\n",
    "    \n",
    "    Testentro, Testsc_err, Testaccu,TestEnc = sess.run([mean_cross_entropy, mean_scaled_error,accuracy, encoded_visual], feed_dict = {visual_in: obs, labels: fLabels})\n",
    "    print('Test:  '+str(Testentro)+ ' - '+str(Testsc_err)+' - '+str(Testaccu)+ ' - Gini: ' + str(np.round(gini(np.abs(TestEnc)),3)))\n",
    "    AllGinis.append(gini(np.abs(TestEnc)))\n",
    "    \n",
    "    if Valaccu >np.max(AllValaccuracies):\n",
    "        saver.save(sess, \"./Results/TowerTraining/Classifier/Model_lr0001_scaled/model.ckpt\")\n",
    "        print('saved model')\n",
    "        LastSave = i-1\n",
    "        \n",
    "    AllValaccuracies.append(Valaccu)\n",
    "    for batchX, batchy in train_it:\n",
    "        batchy = getRealLabel(batchy,realLabel)\n",
    "        entropies = []\n",
    "        accuracies = []\n",
    "        scaled_errs = []\n",
    "        entro, sc_err, accu,comp, _ = sess.run([mean_cross_entropy, mean_scaled_error,accuracy, comparison,training_step], feed_dict = {visual_in: batchX, labels: batchy})\n",
    "        entropies.append(entro)\n",
    "        accuracies.append(accu)\n",
    "        scaled_errs.append(sc_err)\n",
    "        #print(str(entro)+' - '+str(accu))\n",
    "        if count>126:\n",
    "            Allentropies.append(entropies)\n",
    "            Allaccuracies.append(accuracies)\n",
    "            Allscaled_errs.append(scaled_errs)\n",
    "            break\n",
    "        count = count+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./Results/TowerTraining/Classifier/Model_lr0001_scaled/Accs.npy\",Allaccuracies)\n",
    "np.save(\"./Results/TowerTraining/Classifier/Model_lr0001_scaled/Entro.npy\",Allentropies)\n",
    "np.save(\"./Results/TowerTraining/Classifier/Model_lr0001_scaled/Err.npy\",Allscaled_errs)\n",
    "np.save(\"./Results/TowerTraining/Classifier/Model_lr0001_scaled/ValAcc.npy\",AllValaccuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Noisy Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoisyLabel(labelBatch,RL,noise_prob):\n",
    "    newLB = []\n",
    "    for label in labelBatch:\n",
    "        if np.random.uniform(0,1,1) < noise_prob:\n",
    "            label = np.round(np.random.uniform(0,len(RL)-1,1))\n",
    "        l = RL[int(label)]\n",
    "        newLB.append(l)\n",
    "    return newLB\n",
    "_,yTrain = train_it.next()\n",
    "print(yTrain[:10])\n",
    "yTrainR = getRealLabel(yTrain[:10],realLabel)\n",
    "yTrainN = getNoisyLabel(yTrain[:10],realLabel,0.2)\n",
    "\n",
    "print(yTrainN)\n",
    "print(yTrainR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T19:43:34.304894Z",
     "start_time": "2020-07-17T18:31:55.035307Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "Allentropies = []\n",
    "Allaccuracies = []\n",
    "AllValaccuracies = []\n",
    "Allscaled_errs = []\n",
    "AllGinis = []\n",
    "LastSave = 0\n",
    "\n",
    "epochs = 30\n",
    "for i in range(epochs):\n",
    "    count = 0\n",
    "    xVal,yVal = val_it.next()\n",
    "    yVal = getRealLabel(yVal,realLabel)\n",
    "    Valentro, Valsc_err, Valaccu,Valcomp = sess.run([mean_cross_entropy, mean_scaled_error,accuracy, comparison], feed_dict = {visual_in: xVal, labels: yVal})\n",
    "    if i==0:\n",
    "        print('Validation:  '+str(Valentro)+ ' - '+str(Valsc_err)+' - '+str(Valaccu))\n",
    "        AllValaccuracies.append(Valaccu)\n",
    "    else:\n",
    "        print('Training: '+str(np.mean(entropies))+' - '+str(np.mean(scaled_errs))+' - '+str(np.mean(accuracies))+\n",
    "          '    Validation:  '+str(Valentro)+ ' - '+str(Valsc_err)+' - '+str(Valaccu))\n",
    "    \n",
    "    Testentro, Testsc_err, Testaccu,TestEnc = sess.run([mean_cross_entropy, mean_scaled_error,accuracy, encoded_visual], feed_dict = {visual_in: obs, labels: fLabels})\n",
    "    print('Test:  '+str(Testentro)+ ' - '+str(Testsc_err)+' - '+str(Testaccu)+ ' - Gini: ' + str(np.round(gini(np.abs(TestEnc)),3)))\n",
    "    AllGinis.append(gini(np.abs(TestEnc)))\n",
    "    #if Valaccu >np.max(AllValaccuracies):\n",
    "    #    saver.save(sess, \"./Results/TowerTraining/Classifier/Model_lr0001/model.ckpt\")\n",
    "    #    print('saved model')\n",
    "    #    LastSave = i-1\n",
    "    AllValaccuracies.append(Valaccu)\n",
    "    for batchX, batchy in train_it:\n",
    "        batchy = getNoisyLabel(batchy,realLabel,0.1)\n",
    "        entropies = []\n",
    "        accuracies = []\n",
    "        scaled_errs = []\n",
    "        entro, sc_err, accu,comp, _ = sess.run([mean_cross_entropy, mean_scaled_error,accuracy, comparison,training_step], feed_dict = {visual_in: batchX, labels: batchy})\n",
    "        entropies.append(entro)\n",
    "        accuracies.append(accu)\n",
    "        scaled_errs.append(sc_err)\n",
    "        #print(str(entro)+' - '+str(accu))\n",
    "        if count>126:\n",
    "            Allentropies.append(entropies)\n",
    "            Allaccuracies.append(accuracies)\n",
    "            Allscaled_errs.append(scaled_errs)\n",
    "            break\n",
    "        count = count+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T12:41:28.202309Z",
     "start_time": "2020-07-06T12:40:47.862998Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "saver.restore(sess, \"./Results/TowerTraining/Classifier/Model_lr0001_scaled/model.ckpt\")\n",
    "act,out,probs,probsO,grad,HG = sess.run([hidden,output,norm_logits,all_log_probs,gradients,hidden_grad], feed_dict = {visual_in: obs*255,labels: fLabels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T12:41:32.114116Z",
     "start_time": "2020-07-06T12:41:32.102148Z"
    }
   },
   "outputs": [],
   "source": [
    "print('After Training: '+str(np.sum(out==fLabels)/(fLabels.shape[0]*4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(path+'ClassOut.npy',out)\n",
    "np.save(path+'ClassAct.npy',act)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "227.85px",
    "left": "1230px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
